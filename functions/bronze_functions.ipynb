{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da436d0c-f55f-489a-b9b4-7d5c4c07ebd6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Função para salvar tabela no catálogo"
    }
   },
   "outputs": [],
   "source": [
    "def save_table(df, table_name, layer):\n",
    "    (\n",
    "        df.write.mode(\"append\").format('delta')\n",
    "        .option(\"mergeSchema\", \"true\")    \n",
    "        .option(\"delta.columnMapping.mode\", \"name\")    \n",
    "        .saveAsTable(f\"{layer}_layer.{table_name}\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9660845-ee01-4a92-929c-f824a0611dd9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Função para realizar a ingestão de dados"
    }
   },
   "outputs": [],
   "source": [
    "def ingestao_dados(arquivos, tipo_arquivos):\n",
    "\n",
    "    for file_path in arquivos:\n",
    "\n",
    "        # Verifica se o tipo de arquivo é um CSV\n",
    "        if(tipo_arquivos == 'csv'):\n",
    "\n",
    "            # Obtém o nome da tabela a partir do nome do arquivo\n",
    "            table_name = file_path.split('/')[5].replace('.csv', '')\n",
    "\n",
    "            # Realiza a leitura dos dados em um dataframe com Spark\n",
    "            dados = spark.read.options(\n",
    "                header=True,\n",
    "                inferSchema=True,\n",
    "                delimiter=\";\"\n",
    "            ).csv(file_path)\n",
    "\n",
    "        else:\n",
    "\n",
    "            # Obtém o nome da tabela a partir do nome do arquivo\n",
    "            table_name = file_path.split('/')[5].replace('.json', '').replace('_json', '')\n",
    "\n",
    "            # Realiza a leitura dos dados em um dataframe com Spark\n",
    "            dados = spark.read.option(\"multiline\", \"true\").json(file_path)  \n",
    "\n",
    "            # Extrai os campos e registros a partir do JSON\n",
    "            colunas = [campo[\"id\"] for campo in dados.select(\"fields\").first()[\"fields\"]]\n",
    "            tipos_colunas = [campo[\"type\"] for campo in dados.select(\"fields\").first()[\"fields\"]]\n",
    "            registros = dados.select(\"records\").first()[\"records\"]\n",
    "\n",
    "            # Cria o dataframe com Spark a partir das informações importadas\n",
    "            dados = spark.createDataFrame(registros, schema=colunas)\n",
    "\n",
    "            # Variável auxiliar para mapear os tipos de dados\n",
    "            mapa_tipos = {\n",
    "                'text': 'string',\n",
    "                'numeric': 'double',\n",
    "                'int': 'int',\n",
    "                'timestamp': 'timestamp'\n",
    "            }\n",
    "\n",
    "            # Ajusta os tipos de dados do dataframe criado a partir do arquivo JSON\n",
    "            for coluna in colunas:\n",
    "\n",
    "                indice = colunas.index(coluna)\n",
    "                tipo_coluna = tipos_colunas[indice]\n",
    "                tipo = mapa_tipos.get(tipo_coluna, 'string')  # default para 'string' se tipo não reconhecido\n",
    "                if coluna in dados.columns:\n",
    "                    dados = dados.withColumn(coluna, col(coluna).cast(tipo))\n",
    "\n",
    "        # Leitura dos dados da tabela existente (caso já tinha sido salva anteriormente)\n",
    "        dados_existentes = spark.read.table(f\"bronze_layer.{table_name}\")\n",
    "        \n",
    "        # Verificando se os dados importados já existem\n",
    "        verifica_tabela = spark.catalog.tableExists(f\"bronze_layer.{table_name}\")\n",
    "\n",
    "        # Se a tabela já existir, realiza filtragem no dataframe importado para inserir apenas os dados novos\n",
    "        if verifica_tabela:         \n",
    "\n",
    "            colunas = dados.columns\n",
    "\n",
    "            condicoes = [dados[c] == dados_existentes[c] for c in colunas]\n",
    "\n",
    "            df_filtrado = dados.join(dados_existentes, on=condicoes, how=\"left_anti\")\n",
    "\n",
    "            df_filtrado = df_filtrado.dropDuplicates()\n",
    "            \n",
    "            print('================================== CARGA DE DADOS ==================================')\n",
    "            print('>>> Camada: Bronze')\n",
    "            print(f'>>> Tipo do arquivo: {tipo_arquivos}')\n",
    "            print(f'>>> Caminho Raw: {file_path}')\n",
    "            print(f'>>> Tabela: {table_name}')\n",
    "            print('>>> Executando carga de dados, aguarde...')\n",
    "\n",
    "            try:        \n",
    "                save_table(df_filtrado, table_name, 'bronze')\n",
    "                print('=========================== CARGA FINALIZADA COM SUCESSO ===========================\\n')\n",
    "            except:\n",
    "                print('============================= ERRO NA CARGA DE DADOS... ============================\\n')    \n",
    "\n",
    "        # Caso a tabela não exista, realiza a carga dos dados pela primeira vez\n",
    "        else:\n",
    "            \n",
    "            print('================================== CARGA DE DADOS ==================================')\n",
    "            print('>> Camada: Bronze')\n",
    "            print(f'>> Tipo do arquivo: {tipo_arquivos}')\n",
    "            print(f'>> Caminho Raw: {file_path}')\n",
    "            print(f'>> Tabela: {table_name}')\n",
    "            print('>>> Executando carga de dados, aguarde...')\n",
    "\n",
    "            dados = dados.dropDuplicates()\n",
    "\n",
    "            try:        \n",
    "                save_table(dados, table_name, 'bronze')\n",
    "                print('=========================== CARGA FINALIZADA COM SUCESSO ===========================\\n')\n",
    "            except:\n",
    "                print('============================= ERRO NA CARGA DE DADOS... ============================\\n')    \n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_functions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}