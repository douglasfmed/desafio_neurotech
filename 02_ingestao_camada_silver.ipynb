{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f632a40-5ce5-44e0-a10a-0c35ea6c814c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Importação das bibliotecas necessárias"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType, TimestampType\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import regexp_replace, col, trim, sum, when, to_timestamp, substring, when, count, year, current_date, upper\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77cc7556-2d62-419a-90e4-0794f4975e78",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Criação do schema bronze no catálogo de dados"
    }
   },
   "outputs": [],
   "source": [
    "print(\">>> Criando schema silver no catálogo de dados, caso ainda não exista...\", end=\" \")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS silver_layer\");\n",
    "print(\"OK!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdb76849-3dd1-48e3-82ee-fe3673bf7db6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Importação das funções da camada silver"
    }
   },
   "outputs": [],
   "source": [
    "%run /Workspace/Users/douglasfmedeiros@outlook.com/desafio_neurotech/functions/silver_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aaf01135-72db-406c-b8b1-a5773b27e58e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Verificação do número de colunas das tabelas"
    }
   },
   "outputs": [],
   "source": [
    "# Lista todas as tabelas existentes na camada bronze\n",
    "tabelas = spark.catalog.listTables(\"bronze_layer\");\n",
    "tabelas = [t for t in tabelas if t.name != \"_sqldf\"]\n",
    "\n",
    "# A primeira análise a ser feita é se todas as tabelas importadas na camada bronze (brutas) possuem a mesma quantidade de colunas\n",
    "print('================================== VERIFICAÇÃO - QTD COLUNAS DAS TABELAS ==================================')\n",
    "print('>>> Verificando o número de colunas de cada tabela...\\n')\n",
    "\n",
    "for tabela in tabelas:\n",
    "\n",
    "    table_name = tabela.name\n",
    "\n",
    "    dados = spark.read.table(f\"bronze_layer.{table_name}\");\n",
    "\n",
    "    # Verificar a quantidade de colunas de cada tabela\n",
    "    print(f'Tabela: {table_name} / Qtd Colunas: {len(dados.columns)}')\n",
    "\n",
    "print('')\n",
    "print('>>> Na tabela IPTU_2024 há uma coluna adicional, vamos investigar se existem colunas diferentes entre as tabelas...')\n",
    "print('===========================================================================================================')\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a038998-cf54-4617-a5ef-a88044bf1e3a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Importando as tabelas da camada bronze"
    }
   },
   "outputs": [],
   "source": [
    "print('>>> Importando todas as tabelas da camada bronze...', end=' ')\n",
    "for tabela in tabelas:\n",
    "    nome_variavel = tabela.name.replace('iptu', 'dados')\n",
    "    globals()[nome_variavel] = spark.read.table(f'bronze_layer.{tabela.name}')\n",
    "\n",
    "dfs = [eval(t.name.replace('iptu', 'dados')) for t in tabelas]\n",
    "nomes = [tabela.name for tabela in tabelas]\n",
    "print('OK!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c55294e-c280-46fc-b38e-ff00c5c92292",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Verificação das colunas exclusivas de cada tabela"
    }
   },
   "outputs": [],
   "source": [
    "print('================================== VERIFICAÇÃO - COLUNAS EXCLUSIVAS ==================================')\n",
    "print('>>> Verificando se há colunas diferentes entre os dataframes...\\n')\n",
    "\n",
    "# Função para encontrar colunas exclusivas de cada DataFrame\n",
    "for i, df_base in enumerate(dfs):\n",
    "    colunas_base = set(df_base.columns)\n",
    "    \n",
    "    # Junta todas as colunas dos outros DataFrames\n",
    "    colunas_outras = set().union(*[set(dfs[j].columns) for j in range(len(dfs)) if j != i])\n",
    "    \n",
    "    # Calcula colunas exclusivas\n",
    "    exclusivas = colunas_base - colunas_outras\n",
    "    \n",
    "    print(f\"Colunas exclusivas de {nomes[i]}: {sorted(exclusivas)}\")\n",
    "\n",
    "print('')\n",
    "print('>>> Como esperado, há colunas adicionais na tabela IPTU_2024, vamos ajustar isso...', end=\" \")\n",
    "\n",
    "# Ajustando colunas da tabela de IPTU de 2024\n",
    "dados_2024 = dados_2024.withColumnRenamed('quant pavimentos', 'quantidade de pavimentos').withColumnRenamed('valor IPTU', 'valor cobrado de IPTU').drop('_id')\n",
    "\n",
    "print('OK!')\n",
    "\n",
    "print('======================================================================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c59850f8-d382-4754-acac-d82326d4bb8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dfs = [eval(t.name.replace('iptu', 'dados')) for t in tabelas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f14425df-6456-4ece-b9ef-b5220c64560e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print('>>> Analisando os tipos de dados das tabelas da camada bronze (inferidas pelo spark), podemos perceber algumas inconsistências, como, por exemplo, a coluna ÁREA CONSTRUÍDA estar como string, quando deveria ser uma informação numérica. Durante o processamento de dados adiante, isso será ajustado para todas as colunas. Os tipos de todas as colunas podem ser observados abaixo: \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f223ef10-dcb6-47d5-af9d-df9cbe9af79d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dados_2024.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13b85086-325b-4bc0-bbdf-4faac2907a0b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Processamento dos dados das tabelas da camada bronze"
    }
   },
   "outputs": [],
   "source": [
    "for df, nome in zip(dfs, nomes):\n",
    "\n",
    "    table_name = nome\n",
    "\n",
    "    print(f'========================================== PROCESSAMENTO DE DADOS [Tabela {table_name}] ==========================================\\n')\n",
    "\n",
    "    # Ajuste na nomenclatura das colunas\n",
    "    print('Ajustando os nomes das colunas...', end=\" \")\n",
    "\n",
    "    df = df.toDF(*[limpar_nome_coluna(c) for c in df.columns])\n",
    "\n",
    "    print('OK!')    \n",
    "\n",
    "    # Removendo espaços (lixo)\n",
    "    print('Removendo espaços desnecessários do conteúdo da tabela...', end=\" \")\n",
    "\n",
    "    df = df.select([\n",
    "        trim(col(c)).alias(c) if isinstance(df.schema[c].dataType, StringType) else col(c)\n",
    "        for c in df.columns\n",
    "    ])\n",
    "\n",
    "    print('OK!')\n",
    "\n",
    "    # Ajustando coluna TIPO_DE_CONTRIBUINTE\n",
    "    df = df.replace([\"\", \" \"], \"Não informado\", subset=[\"TIPO_DE_CONTRIBUINTE\", \"COMPLEMENTO\"])\n",
    "\n",
    "    # Ajustando o nome do bairro\n",
    "    df = df.withColumn(\"BAIRRO\", upper(df[\"BAIRRO\"]))\n",
    "    \n",
    "    print('Ajustando os tipos de dados...', end=\" \")\n",
    "    # Ajustando os tipos de dados\n",
    "    df = (\n",
    "        df.withColumn('NUMERO_DO_CONTRIBUINTE', col('NUMERO_DO_CONTRIBUINTE').cast('string'))\n",
    "        .withColumn('ANO_DO_EXERCICIO', col('ANO_DO_EXERCICIO').try_cast('double').try_cast('integer'))\n",
    "        #.withColumn('DATA_DO_CADASTRAMENTO', col('DATA_DO_CADASTRAMENTO').cast('string'))\n",
    "        .withColumn('TIPO_DE_CONTRIBUINTE', col('TIPO_DE_CONTRIBUINTE').cast('string'))\n",
    "        .withColumn('CPFCNPJ_MASCARADO_DO_CONTRIBUINTE', col('CPFCNPJ_MASCARADO_DO_CONTRIBUINTE').cast('string'))   \n",
    "        .withColumn('LOGRADOURO', col('LOGRADOURO').cast('string'))   \n",
    "        .withColumn('NUMERO', col('NUMERO').try_cast('double').try_cast('integer'))  \n",
    "        .withColumn('COMPLEMENTO', col('COMPLEMENTO').cast('string')) \n",
    "        .withColumn('BAIRRO', col('BAIRRO').cast('string')) \n",
    "        .withColumn('CIDADE', col('CIDADE').cast('string')) \n",
    "        .withColumn('ESTADO', col('ESTADO').cast('string')) \n",
    "        .withColumn('FRACAO_IDEAL', regexp_replace(col('FRACAO_IDEAL'), \",\", \".\").try_cast('double')) \n",
    "        .withColumn('AREA_TERRENO', regexp_replace(col('AREA_TERRENO'), \",\", \".\").try_cast('double')) \n",
    "        .withColumn('AREA_CONSTRUIDA', regexp_replace(col('AREA_CONSTRUIDA'), \",\", \".\").try_cast('double')) \n",
    "        .withColumn('AREA_OCUPADA', regexp_replace(col('AREA_OCUPADA'), \",\", \".\").try_cast('double')) \n",
    "        .withColumn('VALOR_DO_M2_DO_TERRENO', regexp_replace(col('VALOR_DO_M2_DO_TERRENO'), \",\", \".\").try_cast('double')) \n",
    "        .withColumn('VALOR_DO_M2_DE_CONSTRUCAO', regexp_replace(col('VALOR_DO_M2_DE_CONSTRUCAO'), \",\", \".\").try_cast('double')) \n",
    "        .withColumn('ANO_DA_CONSTRUCAO_CORRIGIDO', col('ANO_DA_CONSTRUCAO_CORRIGIDO').try_cast('double').try_cast('integer')) \n",
    "        .withColumn('QUANTIDADE_DE_PAVIMENTOS', col('QUANTIDADE_DE_PAVIMENTOS').try_cast('double').try_cast('integer')) \n",
    "        .withColumn('TIPO_DE_USO_DO_IMOVEL', col('TIPO_DE_USO_DO_IMOVEL').cast('string')) \n",
    "        .withColumn('TIPO_DE_PADRAO_DA_CONSTRUCAO', col('TIPO_DE_PADRAO_DA_CONSTRUCAO').cast('string')) \n",
    "        .withColumn('FATOR_DE_OBSOLESCENCIA', regexp_replace(col('FATOR_DE_OBSOLESCENCIA'), \",\", \".\").try_cast('double')) \n",
    "        .withColumn('ANO_E_MES_DE_INICIO_DA_CONTRIBUICAO', col('ANO_E_MES_DE_INICIO_DA_CONTRIBUICAO').cast('string')) \n",
    "        .withColumn('VALOR_TOTAL_DO_IMOVEL_ESTIMADO', regexp_replace(col('VALOR_TOTAL_DO_IMOVEL_ESTIMADO'), \",\", \".\").try_cast('double')) \n",
    "        .withColumn('VALOR_COBRADO_DE_IPTU', regexp_replace(col('VALOR_COBRADO_DE_IPTU'), \",\", \".\").try_cast('double')) \n",
    "        .withColumn('CEP', col('CEP').try_cast('double').try_cast('integer')) \n",
    "        .withColumn('REGIME_DE_TRIBUTACAO_DO_IPTU', col('REGIME_DE_TRIBUTACAO_DO_IPTU').cast('string')) \n",
    "        .withColumn('REGIME_DE_TRIBUTACAO_DA_TRSD', col('REGIME_DE_TRIBUTACAO_DA_TRSD').cast('string')) \n",
    "        .withColumn('TIPO_DE_CONSTRUCAO', col('TIPO_DE_CONSTRUCAO').cast('string')) \n",
    "        .withColumn('TIPO_DE_EMPREENDIMENTO', col('TIPO_DE_EMPREENDIMENTO').cast('string')) \n",
    "        .withColumn('TIPO_DE_ESTRUTURA', col('TIPO_DE_ESTRUTURA').cast('string')) \n",
    "        .withColumn('CODIGO_LOGRADOURO', col('CODIGO_LOGRADOURO').try_cast('double').try_cast('integer')) \n",
    "    )\n",
    "    print('OK!')  \n",
    "\n",
    "    # Ajustar tipo de dados da data (apenas para os casos em que é string)  \n",
    "    if(isinstance(df.schema[\"DATA_DO_CADASTRAMENTO\"].dataType, StringType)):\n",
    "        print('Ajustando o formato de dados da coluna DATA_DO_CADASTRAMENTO...', end=' ')\n",
    "        \n",
    "        df = df.withColumn('DATA_DO_CADASTRAMENTO', to_timestamp(substring(col(\"DATA_DO_CADASTRAMENTO\"), 1, 23), \"yyyy/MM/dd HH:mm:ss.SSS\"))\n",
    "    \n",
    "        print('OK!')\n",
    "\n",
    "    # Verificando dados duplicados\n",
    "    print('Removendo dados duplicados...', end=' ')\n",
    "    df = df.dropDuplicates()\n",
    "    print('OK!')\n",
    "\n",
    "    print('\\n------------------------------------ ANÁLISE DE REGISTROS NULOS ------------------------------------\\n')\n",
    "    \n",
    "    print('Analisando a quantidade de registros nulos por coluna...\\n')\n",
    "\n",
    "    analise_nulos(df)\n",
    "\n",
    "    print('\\n>>> Como a quantidade de registros nulosé inferior a 1%, vamos remover os registros nulos')\n",
    "\n",
    "    # Removendo linhas que possuem valores nulos\n",
    "    print('Removendo registros com valores nulos...', end=\" \")\n",
    "    df = df.na.drop()  \n",
    "    print('OK!')\n",
    "\n",
    "    print('\\n---------------------------------- ANÁLISE DE QUALIDADE DOS DADOS ----------------------------------\\n')\n",
    "    \n",
    "    # Análise da qualidade dos dados\n",
    "    df = analise_qualidade_dados(df)    \n",
    "\n",
    "    print('\\n---------------------------------------- INGESTÃO DOS DADOS ----------------------------------------\\n')\n",
    "\n",
    "    ingestao_dados(df, table_name, 'silver')\n",
    "\n",
    "    print('==================================================== PROCESSAMENTO CONCLUÍDO ==================================================\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41d177cd-86b4-4fcd-b61c-dd3a340a9f12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f'////////////////////////////////////////// INGESTÃO DA CAMADA SILVER FINALIZADA //////////////////////////////////////////')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7466145535026667,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "02_ingestao_camada_silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}